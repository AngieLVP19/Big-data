{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "PATH = '/Data'\n",
    "DIR_DATA = '../Data/'\n",
    "sys.path.append(PATH) if PATH not in list(sys.path) else None\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, plot_confusion_matrix,confusion_matrix,cohen_kappa_score,mean_squared_error\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = DIR_DATA + 'DATA FOR CEREBRAL PALSY.csv'\n",
    "data = pd.read_csv(filename, sep=';', decimal=',', header=None, names=['Logitud de zancada (m)', 'Cadencia (paso/min)', 'Longitud de la pierna (m)', 'Edad (años)','Status'] )\n",
    "data\n",
    "#Status:\n",
    "#1: Intact children, (control group)\n",
    "#2: Children with spastic diplegia form of cerebral palsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "The dataset contains cases from a study that was conducted in the Motion Analysis Laboratory at the University of Virginia. Eighty eight children with spastic diplegia form of cerebral palsy (ranging from 2 to 20 years with a mean of 9.9 years) and a neurologically intact control group of 68 children (ranging from 2 to 13 years with a mean of 7.1 years) with no history of motor pathology. Each child performed at leats three walking trials at a freely selected and comfortable walking speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "\n",
    "1. Stride length (numerical)\n",
    "2. Cadence (numerical)\n",
    "3. Leg length (numerical)\n",
    "4. Age (numerical)\n",
    "5. Health status (class attribute)\n",
    "    1 = Neurologically intact child, \n",
    "    2 = Child with the spastic diplegia form of cerebral palsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# describe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "print('*'*65)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = data.groupby(\"Status\", as_index=False).count()\n",
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = cs['Status'].values, y = cs['Logitud de zancada (m)'].values, alpha=0.8)\n",
    "plt.title('Status Frequency')\n",
    "plt.ylabel('Frecuency', fontsize=10)\n",
    "plt.xlabel('Status', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = data.groupby(\"Edad (años)\",as_index=False).count()\n",
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = cs['Edad (años)'].values, y = cs['Logitud de zancada (m)'].values, alpha=0.8)\n",
    "plt.title('Age Frequency')\n",
    "plt.ylabel('Frecuency', fontsize=10)\n",
    "plt.xlabel('Ages', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions plot by Stride length\n",
    "sns.FacetGrid(data, hue='Status', height=5).map(sns.distplot, 'Logitud de zancada (m)').add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions plot by Cadence\n",
    "sns.FacetGrid(data, hue='Status', height=5).map(sns.distplot, 'Cadencia (paso/min)').add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions plot by Leg length\n",
    "sns.FacetGrid(data, hue='Status', height=5).map(sns.distplot, 'Longitud de la pierna (m)').add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions plot by Age\n",
    "sns.FacetGrid(data, hue='Status', height=5).map(sns.distplot, 'Edad (años)').add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bind_edges = np.histogram(data['Edad (años)'], bins=10, density=True)\n",
    "plt.xlabel('Edad (años)')\n",
    "print(counts,sum(counts))\n",
    "pf = counts/sum(counts)\n",
    "print('pf=',pf)\n",
    "print('bind_edges', bind_edges)\n",
    "cdf = np.cumsum(pf)\n",
    "plt.plot(bind_edges[1:], pf)\n",
    "plt.plot(bind_edges[1:], cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Status', y='Edad (años)', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Status', y='Logitud de zancada (m)', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='Status', y='Cadencia (paso/min)', data=data, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='Status', y='Longitud de la pierna (m)', data=data, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue = 'Status' , height = 6).map(plt.scatter,'Edad (años)','Logitud de zancada (m)').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue = 'Status' , height = 6).map(plt.scatter,'Edad (años)','Cadencia (paso/min)').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue = 'Status' , height = 6).map(plt.scatter,'Edad (años)','Longitud de la pierna (m)').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='Status', height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "data.plot(kind='box', subplots=True, layout=(2,5), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramas\n",
    "data.drop(['Status'],axis=1).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1, 1, figsize=(14,8))\n",
    "hm = sns.heatmap(data.corr(), ax=ax, cmap=\"bwr\", annot=True, fmt='.2f', linewidths=.05)\n",
    "fig.subplots_adjust(top=0.93)\n",
    "fig.suptitle('Combined undergone Cerebral palsy Attributes and their Correlation Heatmap', fontsize=14, fontweight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:4].values\n",
    "y = data.iloc[:,4].values\n",
    "\"\"\"# Binarize the output\n",
    "y = label_binarize(y, classes=[1, 2])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label_dict = {1: 'Neurologically intact child',\n",
    "              2: 'Child with the spastic diplegia form of cerebral palsy'}\n",
    "\n",
    "feature_dict = {0: 'Logitud de zancada (m)',\n",
    "                1: 'Cadencia (paso/min)',\n",
    "                2: 'Longitud de la pierna (m)',\n",
    "                3: 'Edad (años)'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=2)\n",
    "Y_sklearn = pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for lab, col in zip((1,2), ('blue', 'red')):\n",
    "        plt.scatter(Y_sklearn[y==lab, 0], Y_sklearn[y==lab, 1], label=lab, c=col)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos los datos en el conjunto de entrenamiento y el conjunto de prueba con  sklearn.model_selection, train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "print(len(x),len(x_test),len(x_train))\n",
    "print(len(y),len(y_test),len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estandarizamos escalas con sklearn.preprocessing, StandardScaler\n",
    "sc=StandardScaler()  \n",
    "x_train=sc.fit_transform(x_train)  #y no porque solo estamos transformando las variabkes independientes, x. Caracteristicas\n",
    "x_test=sc.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_train=pca.fit_transform(x_train)\n",
    "x_test=pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "log_reg = LogisticRegression()\n",
    "new_X_train = kmeans.fit_transform(x_train)\n",
    "log_reg.fit(new_X_train, y_train) \n",
    "y_pred=log_reg.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando Regresion logistica\n",
    "clasificador=LogisticRegression(random_state=42)\n",
    "clasificador.fit(x_train,y_train) #entrenando el clasificador\n",
    "y_pred=clasificador.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
    "k=cohen_kappa_score(y_test, y_pred, labels=None, weights=None, sample_weight=None)\n",
    "mse=mean_squared_error(y_test, y_pred)\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('kappa: ', k)\n",
    "print('mse: ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "plot_confusion_matrix(clasificador,x_test,y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set=x_test,y_test\n",
    "X1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n",
    "                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\n",
    "plt.contourf(X1,X2,clasificador.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n",
    "            alpha=0.75,cmap=ListedColormap(('red','green')))\n",
    "plt.xlim(X1.min(),X1.max())\n",
    "plt.ylim(X2.min(),X2.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j,0], X_set[y_set==j,1],\n",
    "               c=ListedColormap(('red','green'))(i),label=j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create dataset\n",
    "#X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "model = clasificador\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=200, n_estimators=200, random_state=42).fit(x_train, y_train) \n",
    "prediction = rf.predict(x_test) \n",
    "\n",
    "f1 = f1_score(y_test, prediction, average=\"macro\")\n",
    "precision = precision_score(y_test, prediction, average=\"macro\")\n",
    "recall = recall_score(y_test, prediction, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, prediction, normalize=True)\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "plot_confusion_matrix(rf,x_test,y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set=x_test,y_test\n",
    "X1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n",
    "                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\n",
    "plt.contourf(X1,X2,rf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n",
    "            alpha=0.75,cmap=ListedColormap(('red','green')))\n",
    "plt.xlim(X1.min(),X1.max())\n",
    "plt.ylim(X2.min(),X2.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j,0], X_set[y_set==j,1],\n",
    "               c=ListedColormap(('red','green'))(i),label=j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"X=data.drop(['Status'], axis=1)\n",
    "y=data['Status']\n",
    " \n",
    "best=SelectKBest(k=2)\n",
    "X_new = best.fit_transform(X, y)\n",
    "X_new.shape\n",
    "selected = best.get_support(indices=True)\n",
    "print(X.columns[selected])\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"used_features =X.columns[selected]\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(data[used_features].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Split dataset in training and test datasets\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=6) \n",
    "y_train =X_train[\"Status\"]\n",
    "y_test = X_test[\"Status\"]\"\"\"\n",
    "\n",
    "# Instantiate the classifier\n",
    "gnb = GaussianNB()\n",
    "# Train classifier\n",
    "\"\"\"gnb.fit(\n",
    "    X_train[used_features].values,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "y_pred = gnb.predict(X_test[used_features])\"\"\"\n",
    "gnb.fit(\n",
    "    x_train,\n",
    "    y_train\n",
    ")\n",
    "y_pred = gnb.predict(x_test)\n",
    " \n",
    "print('Accuracy en el set de Entrenamiento: {:.2f}'\n",
    "     .format(gnb.score(x_train, y_train)))\n",
    "print('Accuracy en el set de Test: {:.2f}'\n",
    "     .format(gnb.score(x_test, y_test)))\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "plot_confusion_matrix(gnb,x_test,y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set=x_test,y_test\n",
    "X1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n",
    "                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\n",
    "plt.contourf(X1,X2,gnb.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n",
    "            alpha=0.75,cmap=ListedColormap(('red','green')))\n",
    "plt.xlim(X1.min(),X1.max())\n",
    "plt.ylim(X2.min(),X2.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j,0], X_set[y_set==j,1],\n",
    "               c=ListedColormap(('red','green'))(i),label=j)\n",
    "plt.title('Naive Bayes (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='rbf', C=1, probability=True).fit(x_train, y_train) \n",
    "prediction = svc.predict(x_test) \n",
    "\n",
    "f1 = f1_score(y_test, prediction, average=\"macro\")\n",
    "precision = precision_score(y_test, prediction, average=\"macro\")\n",
    "recall = recall_score(y_test, prediction, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, prediction, normalize=True)\n",
    "print('Svm:')\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split                                           \n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(x_train, y_train)\n",
    "prediction2=clf.predict(x_test)\n",
    "f1 = f1_score(y_test, prediction2, average=\"macro\")\n",
    "precision = precision_score(y_test, prediction2, average=\"macro\")\n",
    "recall = recall_score(y_test, prediction2, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, prediction2, normalize=True)\n",
    "print('MLP:')\n",
    "print('F1: ',f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
